{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0490a7",
   "metadata": {},
   "source": [
    "# FGSD Graph Embedding Evaluation on REDDIT-MULTI-12K Dataset\n",
    "\n",
    "This notebook evaluates the FGSD (Family of Graph Spectral Distances) method on the REDDIT-MULTI-12K dataset using various classifiers.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab (GPU runtime recommended)\n",
    "- Internet connection for dataset download\n",
    "\n",
    "**Runtime Setup:**\n",
    "1. Go to Runtime ‚Üí Change runtime type\n",
    "2. Select GPU as Hardware accelerator\n",
    "3. Click Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f86b0a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ef9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with compatible versions for Google Colab\n",
    "!pip install -q \"numpy>=1.26.0,<2.2.0\"\n",
    "!pip install -q \"scipy>=1.11.0\"\n",
    "!pip install -q \"networkx>=3.0\"\n",
    "!pip install -q \"scikit-learn>=1.3.0\"\n",
    "!pip install -q pandas\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "\n",
    "print(\"‚úì All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d28e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import tracemalloc\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f6881",
   "metadata": {},
   "source": [
    "## 2. FGSD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13907f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Estimator:\n",
    "    \"\"\"Base estimator class.\"\"\"\n",
    "    def _set_seed(self):\n",
    "        np.random.seed(self.seed)\n",
    "    \n",
    "    def _check_graphs(self, graphs):\n",
    "        return graphs\n",
    "\n",
    "class FGSD(Estimator):\n",
    "    r\"\"\"An implementation of FGSD from NeurIPS '17.\n",
    "    \n",
    "    The procedure calculates the Moore-Penrose spectrum of the normalized Laplacian.\n",
    "    Using this spectrum the histogram of the spectral features is used as a whole graph representation.\n",
    "\n",
    "    Args:\n",
    "        hist_bins (int): Number of histogram bins. Default is 200.\n",
    "        hist_range (int): Histogram range considered. Default is 20.\n",
    "        seed (int): Random seed value. Default is 42.\n",
    "        regularization (float): Regularization factor for numerical stability. Default is 1e-10.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hist_bins: int = 200, hist_range: int = 20, seed: int = 42, regularization: float = 1e-10):\n",
    "        self.hist_bins = hist_bins\n",
    "        self.hist_range = (0, hist_range)\n",
    "        self.seed = seed\n",
    "        self.regularization = regularization\n",
    "        self.failed_indices = []\n",
    "\n",
    "    def _calculate_fgsd(self, graph, graph_idx=None):\n",
    "        \"\"\"Calculate the features of a graph with error handling.\"\"\"\n",
    "        try:\n",
    "            L = nx.normalized_laplacian_matrix(graph).todense()\n",
    "            \n",
    "            # Add regularization for numerical stability\n",
    "            L_reg = L + self.regularization * np.eye(L.shape[0])\n",
    "            \n",
    "            # Try standard pinv first\n",
    "            try:\n",
    "                fL = np.linalg.pinv(L_reg, rcond=1e-10)\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Fallback: use eigenvalue decomposition with regularization\n",
    "                if graph_idx is not None:\n",
    "                    print(f\"  ‚ö†Ô∏è  SVD failed for graph {graph_idx}, using eigenvalue fallback...\")\n",
    "                eigenvalues, eigenvectors = np.linalg.eigh(L_reg)\n",
    "                # Filter out near-zero eigenvalues\n",
    "                threshold = 1e-10\n",
    "                eigenvalues_inv = np.where(np.abs(eigenvalues) > threshold, 1.0 / eigenvalues, 0)\n",
    "                fL = eigenvectors @ np.diag(eigenvalues_inv) @ eigenvectors.T\n",
    "            \n",
    "            ones = np.ones(L.shape[0])\n",
    "            S = np.outer(np.diag(fL), ones) + np.outer(ones, np.diag(fL)) - 2 * fL\n",
    "            \n",
    "            # Ensure S values are non-negative (can have small negative values due to numerical errors)\n",
    "            S = np.maximum(S, 0)\n",
    "            \n",
    "            hist, _ = np.histogram(S.flatten(), bins=self.hist_bins, range=self.hist_range)\n",
    "            return hist\n",
    "            \n",
    "        except Exception as e:\n",
    "            if graph_idx is not None:\n",
    "                print(f\"  ‚ùå Error processing graph {graph_idx}: {str(e)}\")\n",
    "                self.failed_indices.append(graph_idx)\n",
    "            # Return zero histogram as fallback\n",
    "            return np.zeros(self.hist_bins)\n",
    "\n",
    "    def fit(self, graphs: List[nx.classes.graph.Graph]):\n",
    "        \"\"\"Fit FGSD model.\"\"\"\n",
    "        self._set_seed()\n",
    "        graphs = self._check_graphs(graphs)\n",
    "        self.failed_indices = []\n",
    "        \n",
    "        print(f\"Processing {len(graphs)} graphs...\")\n",
    "        self._embedding = []\n",
    "        for idx, graph in enumerate(graphs):\n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(graphs)} graphs...\")\n",
    "            self._embedding.append(self._calculate_fgsd(graph, idx))\n",
    "        \n",
    "        if self.failed_indices:\n",
    "            print(f\"‚ö†Ô∏è  Warning: {len(self.failed_indices)} graphs failed processing\")\n",
    "\n",
    "    def get_embedding(self) -> np.array:\n",
    "        \"\"\"Get the embedding of graphs.\"\"\"\n",
    "        return np.array(self._embedding)\n",
    "\n",
    "    def infer(self, graphs: List[nx.classes.graph.Graph]) -> np.array:\n",
    "        \"\"\"Infer the embedding for a list of graphs.\"\"\"\n",
    "        self._set_seed()\n",
    "        graphs = self._check_graphs(graphs)\n",
    "        \n",
    "        print(f\"Inferring {len(graphs)} graphs...\")\n",
    "        embedding = []\n",
    "        for idx, graph in enumerate(graphs):\n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(graphs)} graphs...\")\n",
    "            embedding.append(self._calculate_fgsd(graph, idx))\n",
    "        \n",
    "        return np.array(embedding)\n",
    "\n",
    "print(\"‚úì FGSD class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2def9a",
   "metadata": {},
   "source": [
    "## 3. Dataset Download and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load_reddit():\n",
    "    \"\"\"Download and load REDDIT-MULTI-12K dataset from TU Dortmund.\"\"\"\n",
    "    data_dir = '/content/REDDIT-MULTI-12K'\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    base_url = 'https://www.chrsmrrs.com/graphkerneldatasets/REDDIT-MULTI-12K.zip'\n",
    "    zip_path = os.path.join(data_dir, 'REDDIT-MULTI-12K.zip')\n",
    "    \n",
    "    # Download if not exists\n",
    "    if not os.path.exists(os.path.join(data_dir, 'REDDIT-MULTI-12K')):\n",
    "        print(\"üì• Downloading REDDIT-MULTI-12K dataset (this may take a while)...\")\n",
    "        urllib.request.urlretrieve(base_url, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"‚úì Download complete.\")\n",
    "    else:\n",
    "        print(\"‚úì Dataset already downloaded.\")\n",
    "    \n",
    "    # Parse dataset files\n",
    "    dataset_path = os.path.join(data_dir, 'REDDIT-MULTI-12K')\n",
    "    \n",
    "    print(\"üìÇ Loading dataset files...\")\n",
    "    graph_indicator = np.loadtxt(os.path.join(dataset_path, 'REDDIT-MULTI-12K_graph_indicator.txt'), dtype=int)\n",
    "    edges = np.loadtxt(os.path.join(dataset_path, 'REDDIT-MULTI-12K_A.txt'), dtype=int, delimiter=',')\n",
    "    graph_labels = np.loadtxt(os.path.join(dataset_path, 'REDDIT-MULTI-12K_graph_labels.txt'), dtype=int)\n",
    "    \n",
    "    print(\"üî® Building NetworkX graphs...\")\n",
    "    num_graphs = len(graph_labels)\n",
    "    graphs = [nx.Graph() for _ in range(num_graphs)]\n",
    "    \n",
    "    # Add nodes\n",
    "    for node_id, graph_id in enumerate(graph_indicator, start=1):\n",
    "        graphs[graph_id - 1].add_node(node_id)\n",
    "    \n",
    "    # Add edges\n",
    "    print(f\"Adding {len(edges)} edges...\")\n",
    "    for i, edge in enumerate(edges):\n",
    "        if i % 100000 == 0:\n",
    "            print(f\"  Processed {i}/{len(edges)} edges...\")\n",
    "        node1, node2 = edge\n",
    "        graph_id = graph_indicator[node1 - 1]\n",
    "        graphs[graph_id - 1].add_edge(node1, node2)\n",
    "    \n",
    "    # Relabel nodes to be contiguous starting from 0\n",
    "    print(\"üè∑Ô∏è  Relabeling nodes...\")\n",
    "    graphs = [nx.convert_node_labels_to_integers(g) for g in graphs]\n",
    "    \n",
    "    # Convert labels to 0-indexed\n",
    "    labels = graph_labels - 1\n",
    "    \n",
    "    print(f\"‚úì Dataset loaded: {len(graphs)} graphs\")\n",
    "    return graphs, labels\n",
    "\n",
    "print(\"‚úì Dataset loading function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a43c149",
   "metadata": {},
   "source": [
    "## 4. Dataset Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(graphs, labels):\n",
    "    \"\"\"Analyze the dataset to understand its properties.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä DATASET ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Number of graphs: {len(graphs)}\")\n",
    "    print(f\"Number of classes: {len(np.unique(labels))}\")\n",
    "    \n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  Class {label}: {count} graphs ({100*count/len(labels):.2f}%)\")\n",
    "    \n",
    "    # Sample graphs for statistics\n",
    "    sample_size = min(1000, len(graphs))\n",
    "    sample_indices = np.random.choice(len(graphs), sample_size, replace=False)\n",
    "    sample_graphs = [graphs[i] for i in sample_indices]\n",
    "    \n",
    "    # Graph statistics\n",
    "    num_nodes = [g.number_of_nodes() for g in sample_graphs]\n",
    "    num_edges = [g.number_of_edges() for g in sample_graphs]\n",
    "    densities = [nx.density(g) if g.number_of_nodes() > 1 else 0 for g in sample_graphs]\n",
    "    \n",
    "    print(f\"\\nüìà Graph statistics (based on {sample_size} samples):\")\n",
    "    print(f\"  Nodes - Min: {min(num_nodes)}, Max: {max(num_nodes)}, Mean: {np.mean(num_nodes):.2f}, Std: {np.std(num_nodes):.2f}\")\n",
    "    print(f\"  Edges - Min: {min(num_edges)}, Max: {max(num_edges)}, Mean: {np.mean(num_edges):.2f}, Std: {np.std(num_edges):.2f}\")\n",
    "    print(f\"  Density - Min: {min(densities):.4f}, Max: {max(densities):.4f}, Mean: {np.mean(densities):.4f}\")\n",
    "    \n",
    "    # Check connectivity\n",
    "    connected = [nx.is_connected(g) for g in sample_graphs]\n",
    "    print(f\"  Connected graphs: {sum(connected)}/{len(sample_graphs)} ({100*sum(connected)/len(sample_graphs):.2f}%)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(unique, counts)\n",
    "    plt.xlabel('Class Label')\n",
    "    plt.ylabel('Number of Graphs')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(num_nodes, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Number of Nodes')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Graph Sizes (Sample)')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Analysis function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496ab3c",
   "metadata": {},
   "source": [
    "## 5. Classifier Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(X_train, X_test, y_train, y_test, classifier_name, clf, use_scaling=True):\n",
    "    \"\"\"Evaluate a single classifier with optional feature scaling.\"\"\"\n",
    "    # Feature scaling\n",
    "    if use_scaling:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "    \n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Prediction\n",
    "    start_time = time.time()\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # AUC\n",
    "    try:\n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_score = clf.predict_proba(X_test_scaled)\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            y_score = clf.decision_function(X_test_scaled)\n",
    "            if len(y_score.shape) == 1:\n",
    "                y_score = y_score.reshape(-1, 1)\n",
    "        else:\n",
    "            y_score = None\n",
    "        \n",
    "        if y_score is not None and y_test_bin.shape[1] > 1:\n",
    "            auc = roc_auc_score(y_test_bin, y_score, average='weighted', multi_class='ovr')\n",
    "        else:\n",
    "            auc = None\n",
    "    except Exception as e:\n",
    "        auc = None\n",
    "    \n",
    "    return {\n",
    "        'classifier': classifier_name,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'train_time': train_time,\n",
    "        'inference_time': inference_time,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "\n",
    "print(\"‚úì Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4156f2",
   "metadata": {},
   "source": [
    "## 6. Load and Analyze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "graphs, labels = download_and_load_reddit()\n",
    "\n",
    "# Analyze dataset\n",
    "analyze_dataset(graphs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363237f9",
   "metadata": {},
   "source": [
    "## 7. Configuration and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804dbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_DIMENSIONS = [100, 200, 400]\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MAX_TRAIN_SIZE = 8000  # Use subset for faster experiments\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration:\")\n",
    "print(f\"  Embedding dimensions to test: {EMBEDDING_DIMENSIONS}\")\n",
    "print(f\"  Test size: {TEST_SIZE * 100}%\")\n",
    "print(f\"  Random state: {RANDOM_STATE}\")\n",
    "print(f\"  Max training size: {MAX_TRAIN_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47356065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use subset for faster experiments\n",
    "if len(graphs) > MAX_TRAIN_SIZE / (1 - TEST_SIZE):\n",
    "    print(f\"\\nüîÑ Using subset of {int(MAX_TRAIN_SIZE / (1 - TEST_SIZE))} graphs for efficiency...\")\n",
    "    indices = np.random.RandomState(RANDOM_STATE).choice(\n",
    "        len(graphs), int(MAX_TRAIN_SIZE / (1 - TEST_SIZE)), replace=False\n",
    "    )\n",
    "    graphs_subset = [graphs[i] for i in indices]\n",
    "    labels_subset = labels[indices]\n",
    "else:\n",
    "    graphs_subset = graphs\n",
    "    labels_subset = labels\n",
    "\n",
    "# Split data\n",
    "graphs_train, graphs_test, y_train, y_test = train_test_split(\n",
    "    graphs_subset, labels_subset, test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, stratify=labels_subset\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set: {len(graphs_train)} graphs\")\n",
    "print(f\"‚úì Test set: {len(graphs_test)} graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09db0be",
   "metadata": {},
   "source": [
    "## 8. Run Experiments with Different Embedding Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "for dim in EMBEDDING_DIMENSIONS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üß™ EVALUATING WITH EMBEDDING DIMENSION: {dim}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Generate embeddings with memory tracking\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"‚ö° Generating embeddings for training set...\")\n",
    "    model = FGSD(hist_bins=dim, hist_range=20, seed=RANDOM_STATE)\n",
    "    model.fit(graphs_train)\n",
    "    X_train = model.get_embedding()\n",
    "    \n",
    "    print(\"‚ö° Generating embeddings for test set...\")\n",
    "    X_test = model.infer(graphs_test)\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_mb = peak / 1024 / 1024\n",
    "    \n",
    "    print(f\"\\nüìä Embedding Statistics:\")\n",
    "    print(f\"  Generation time: {generation_time:.2f}s\")\n",
    "    print(f\"  Peak memory usage: {memory_mb:.2f} MB\")\n",
    "    print(f\"  Train embeddings shape: {X_train.shape}\")\n",
    "    print(f\"  Test embeddings shape: {X_test.shape}\")\n",
    "    \n",
    "    # Check variance\n",
    "    variance = np.var(X_train, axis=0)\n",
    "    print(f\"  Feature variance - Min: {np.min(variance):.4f}, Max: {np.max(variance):.4f}, Mean: {np.mean(variance):.4f}\")\n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        'SVM (Linear)': SVC(kernel='linear', C=1.0, random_state=RANDOM_STATE, probability=True),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=15, \n",
    "                                               random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5,\n",
    "                                                       random_state=RANDOM_STATE)\n",
    "    }\n",
    "    \n",
    "    # Evaluate classifiers\n",
    "    print(f\"\\nüéØ Evaluating Classifiers:\")\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        print(f\"\\n  Testing {clf_name}...\", end=' ')\n",
    "        result = evaluate_classifier(X_train, X_test, y_train, y_test, clf_name, clf)\n",
    "        result['embedding_dim'] = dim\n",
    "        result['generation_time'] = generation_time\n",
    "        result['memory_mb'] = memory_mb\n",
    "        all_results.append(result)\n",
    "        \n",
    "        print(\"‚úì\")\n",
    "        print(f\"    Accuracy: {result['accuracy']:.4f}\")\n",
    "        print(f\"    F1-Score: {result['f1_score']:.4f}\")\n",
    "        if result['auc'] is not None:\n",
    "            print(f\"    AUC: {result['auc']:.4f}\")\n",
    "        print(f\"    Training time: {result['train_time']:.4f}s\")\n",
    "        print(f\"    Inference time: {result['inference_time']:.4f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcff4c",
   "metadata": {},
   "source": [
    "## 9. Results Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da57a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame([{k: v for k, v in r.items() if k != 'y_pred'} for r in all_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìã SUMMARY OF RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('/content/fgsd_reddit_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to /content/fgsd_reddit_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax = axes[0, 0]\n",
    "for clf_name in results_df['classifier'].unique():\n",
    "    clf_data = results_df[results_df['classifier'] == clf_name]\n",
    "    ax.plot(clf_data['embedding_dim'], clf_data['accuracy'], marker='o', label=clf_name)\n",
    "ax.set_xlabel('Embedding Dimension')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy vs Embedding Dimension')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score comparison\n",
    "ax = axes[0, 1]\n",
    "for clf_name in results_df['classifier'].unique():\n",
    "    clf_data = results_df[results_df['classifier'] == clf_name]\n",
    "    ax.plot(clf_data['embedding_dim'], clf_data['f1_score'], marker='s', label=clf_name)\n",
    "ax.set_xlabel('Embedding Dimension')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('F1-Score vs Embedding Dimension')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "ax = axes[1, 0]\n",
    "for clf_name in results_df['classifier'].unique():\n",
    "    clf_data = results_df[results_df['classifier'] == clf_name]\n",
    "    ax.plot(clf_data['embedding_dim'], clf_data['train_time'], marker='^', label=clf_name)\n",
    "ax.set_xlabel('Embedding Dimension')\n",
    "ax.set_ylabel('Training Time (s)')\n",
    "ax.set_title('Training Time vs Embedding Dimension')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Memory usage\n",
    "ax = axes[1, 1]\n",
    "dims = results_df.groupby('embedding_dim')['memory_mb'].first()\n",
    "ax.bar(dims.index, dims.values, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Embedding Dimension')\n",
    "ax.set_ylabel('Memory (MB)')\n",
    "ax.set_title('Peak Memory Usage vs Embedding Dimension')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üèÜ BEST RESULTS PER METRIC\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_acc = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "best_f1 = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "fastest_gen = results_df.loc[results_df['generation_time'].idxmin()]\n",
    "fastest_train = results_df.loc[results_df['train_time'].idxmin()]\n",
    "\n",
    "print(f\"ü•á Best Accuracy: {best_acc['accuracy']:.4f}\")\n",
    "print(f\"   Classifier: {best_acc['classifier']}\")\n",
    "print(f\"   Embedding Dim: {best_acc['embedding_dim']}\")\n",
    "\n",
    "print(f\"\\nü•á Best F1-Score: {best_f1['f1_score']:.4f}\")\n",
    "print(f\"   Classifier: {best_f1['classifier']}\")\n",
    "print(f\"   Embedding Dim: {best_f1['embedding_dim']}\")\n",
    "\n",
    "print(f\"\\n‚ö° Fastest Embedding Generation: {fastest_gen['generation_time']:.2f}s\")\n",
    "print(f\"   Embedding Dim: {fastest_gen['embedding_dim']}\")\n",
    "\n",
    "print(f\"\\n‚ö° Fastest Training: {fastest_train['train_time']:.4f}s\")\n",
    "print(f\"   Classifier: {fastest_train['classifier']}\")\n",
    "print(f\"   Embedding Dim: {fastest_train['embedding_dim']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb68222",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_result = all_results[results_df['accuracy'].idxmax()]\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, best_result['y_pred'])\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title(f'Confusion Matrix - {best_result[\"classifier\"]} (dim={best_result[\"embedding_dim\"]})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìä Classification Report for Best Model:\")\n",
    "print(f\"Classifier: {best_result['classifier']}\")\n",
    "print(f\"Embedding Dimension: {best_result['embedding_dim']}\")\n",
    "print(\"\\n\" + classification_report(y_test, best_result['y_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39564ce2",
   "metadata": {},
   "source": [
    "## 11. Download Results\n",
    "\n",
    "You can download the results CSV file from the Files panel on the left, or use the code below to download it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download results\n",
    "files.download('/content/fgsd_reddit_results.csv')\n",
    "print(\"‚úì Results file ready for download!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
