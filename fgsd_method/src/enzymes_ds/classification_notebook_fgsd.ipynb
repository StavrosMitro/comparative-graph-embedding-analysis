{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf45db1",
   "metadata": {},
   "source": [
    "# FGSD Experiment with Node Labels\n",
    "This notebook runs the Flexible FGSD and Hybrid FGSD experiments on the ENZYMES dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf23c45",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7a8e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added to path: /home/stavros/emb3/fgsd_method/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "print(f\"✅ Added to path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7be5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import tracemalloc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import networkx as nx\n",
    "from karateclub.estimator import Estimator\n",
    "import warnings\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from fgsd import FlexibleFGSD\n",
    "from optimized_method import HybridFGSD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32519ab",
   "metadata": {},
   "source": [
    "## Download and Load ENZYMES Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879ecea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_and_load_enzymes():\n",
    "    data_dir = '/tmp/ENZYMES'\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    base_url = 'https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip'\n",
    "    zip_path = os.path.join(data_dir, 'ENZYMES.zip')\n",
    "\n",
    "    if not os.path.exists(os.path.join(data_dir, 'ENZYMES')):\n",
    "        print(\"Downloading ENZYMES dataset...\")\n",
    "        urllib.request.urlretrieve(base_url, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "    dataset_path = os.path.join(data_dir, 'ENZYMES')\n",
    "\n",
    "    graph_indicator = np.loadtxt(os.path.join(dataset_path, 'ENZYMES_graph_indicator.txt'), dtype=int)\n",
    "    edges = np.loadtxt(os.path.join(dataset_path, 'ENZYMES_A.txt'), dtype=int, delimiter=',')\n",
    "    graph_labels = np.loadtxt(os.path.join(dataset_path, 'ENZYMES_graph_labels.txt'), dtype=int)\n",
    "    node_labels_raw = np.loadtxt(os.path.join(dataset_path, 'ENZYMES_node_labels.txt'), dtype=int)\n",
    "\n",
    "    num_graphs = len(graph_labels)\n",
    "    graphs = [nx.Graph() for _ in range(num_graphs)]\n",
    "    node_labels_list = []\n",
    "\n",
    "    for node_id, graph_id in enumerate(graph_indicator, start=1):\n",
    "        graphs[graph_id - 1].add_node(node_id)\n",
    "\n",
    "    current_idx = 0\n",
    "    for i in range(1, num_graphs + 1):\n",
    "        count = np.sum(graph_indicator == i)\n",
    "        labels_of_graph = node_labels_raw[current_idx : current_idx + count]\n",
    "        node_labels_list.append(labels_of_graph)\n",
    "        current_idx += count\n",
    "\n",
    "    for node1, node2 in edges:\n",
    "        graph_id = graph_indicator[node1 - 1]\n",
    "        graphs[graph_id - 1].add_edge(node1, node2)\n",
    "\n",
    "    graphs = [nx.convert_node_labels_to_integers(g) for g in graphs]\n",
    "    labels = graph_labels - 1\n",
    "\n",
    "    return graphs, labels, node_labels_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee977269",
   "metadata": {},
   "source": [
    "## Create Node Label Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa67873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_node_label_features(node_labels_list):\n",
    "    all_labels = np.concatenate(node_labels_list)\n",
    "    unique_labels = np.unique(all_labels)\n",
    "    n_unique = len(unique_labels)\n",
    "    min_lbl, max_lbl = min(unique_labels), max(unique_labels)\n",
    "\n",
    "    print(f\"Node Labels Found: {unique_labels} (Total unique: {n_unique})\")\n",
    "\n",
    "    features = []\n",
    "    for labels in node_labels_list:\n",
    "        hist, _ = np.histogram(labels, bins=n_unique, range=(min_lbl, max_lbl + 1))\n",
    "        features.append(hist)\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33995cf",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0812356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_classifier(X_train, X_test, y_train, y_test, classifier_name, clf):\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    try:\n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_score = clf.predict_proba(X_test)\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            y_score = clf.decision_function(X_test)\n",
    "            if len(y_score.shape) == 1:\n",
    "                y_score = y_score.reshape(-1, 1)\n",
    "        else:\n",
    "            y_score = None\n",
    "\n",
    "        if y_score is not None and y_test_bin.shape[1] > 1:\n",
    "            auc = roc_auc_score(y_test_bin, y_score, average='weighted', multi_class='ovr')\n",
    "        else:\n",
    "            auc = None\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    return {\n",
    "        'classifier': classifier_name,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'train_time': train_time,\n",
    "        'inference_time': inference_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1b255",
   "metadata": {},
   "source": [
    "## Run FGSD Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6886b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(configs, test_size=0.15, random_state=42):\n",
    "    print(\"Loading ENZYMES dataset...\")\n",
    "    graphs, labels, node_labels_list = download_and_load_enzymes()\n",
    "\n",
    "    X_node_labels = create_node_label_features(node_labels_list)\n",
    "    print(f\"Node Label Features Shape: {X_node_labels.shape}\")\n",
    "\n",
    "    graphs_train, graphs_test, y_train, y_test, X_labels_train, X_labels_test = train_test_split(\n",
    "        graphs, labels, X_node_labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, config in enumerate(configs):\n",
    "        func = config['func']\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        if func == 'hybrid':\n",
    "            harm_bins = config.get('harm_bins', 200)\n",
    "            harm_range = config.get('harm_range', 33)\n",
    "            pol_bins = config.get('pol_bins', 70)\n",
    "            pol_range = config.get('pol_range', 4.1)\n",
    "            print(f\"Experiment {i+1}/{len(configs)}: Function='{func}' + Node Labels\")\n",
    "        else:\n",
    "            bins = config['bins']\n",
    "            rng = config['range']\n",
    "            print(f\"Experiment {i+1}/{len(configs)}: Function='{func}', Bins={bins}, Range={rng} + Node Labels\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()\n",
    "\n",
    "        if func == 'hybrid':\n",
    "            model = HybridFGSD(\n",
    "                harm_bins=harm_bins, harm_range=harm_range,\n",
    "                pol_bins=pol_bins, pol_range=pol_range,\n",
    "                func_type='hybrid', seed=random_state\n",
    "            )\n",
    "        else:\n",
    "            model = FlexibleFGSD(hist_bins=bins, hist_range=rng, func_type=func, seed=random_state)\n",
    "\n",
    "        model.fit(graphs_train)\n",
    "        X_train_spectral = model.get_embedding()\n",
    "        X_test_spectral = model.infer(graphs_test)\n",
    "\n",
    "        X_train = np.hstack([X_train_spectral, X_labels_train])\n",
    "        X_test = np.hstack([X_test_spectral, X_labels_test])\n",
    "\n",
    "        generation_time = time.time() - start_time\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        print(f\"Total Embedding Shape (Spectral + Labels): {X_train.shape}\")\n",
    "\n",
    "        classifiers = {\n",
    "            'SVM (RBF) + Scaler': make_pipeline(\n",
    "                StandardScaler(),\n",
    "                SVC(kernel='rbf', C=100, gamma='scale', probability=True, random_state=random_state)\n",
    "            ),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=1000, random_state=random_state),\n",
    "            'MLP': make_pipeline(\n",
    "                StandardScaler(), \n",
    "                MLPClassifier(\n",
    "                    hidden_layer_sizes=(1024, 512, 256, 128), \n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    alpha=0.001,\n",
    "                    learning_rate_init=0.001,\n",
    "                    learning_rate='adaptive',\n",
    "                    max_iter=2000, \n",
    "                    early_stopping=True,      \n",
    "                    n_iter_no_change=20,      \n",
    "                    random_state=random_state\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            res = evaluate_classifier(X_train, X_test, y_train, y_test, clf_name, clf)\n",
    "            res.update(config)\n",
    "            res['generation_time'] = generation_time\n",
    "            results.append(res)\n",
    "            print(f\"  -> {clf_name}: Train Acc={res['train_accuracy']:.4f}, Test Acc={res['accuracy']:.4f}, F1={res['f1_score']:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838424d",
   "metadata": {},
   "source": [
    "## Summary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c997ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_summary(results):\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"SUMMARY OF RESULTS\")\n",
    "    print(\"=\"*120)\n",
    "    print(f\"{'Func':<12} {'Parameters':<30} {'Classifier':<20} {'Train Acc':<11} {'Test Acc':<10} {'F1':<10} {'GenTime':<8}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "    for r in sorted_results:\n",
    "        if r['func'] == 'hybrid':\n",
    "            params = f\"h_bins={r.get('harm_bins')},h_range={r.get('harm_range')},p_bins={r.get('pol_bins')},p_range={r.get('pol_range')}\"\n",
    "        else:\n",
    "            params = f\"bins={r.get('bins')}, range={r.get('range')}\"\n",
    "\n",
    "        print(f\"{r['func']:<12} {params:<30} {r['classifier']:<20} \"\n",
    "              f\"{r['train_accuracy']:<11.4f} {r['accuracy']:<10.4f} {r['f1_score']:<10.4f} {r['generation_time']:<8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58d7fb",
   "metadata": {},
   "source": [
    "## Run the Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba1a275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multi-Configuration FGSD Experiment...\n",
      "Loading ENZYMES dataset...\n",
      "Node Labels Found: [1 2 3] (Total unique: 3)\n",
      "Node Label Features Shape: (600, 3)\n",
      "\n",
      "================================================================================\n",
      "Experiment 1/2: Function='hybrid' + Node Labels\n",
      "================================================================================\n",
      "Total Embedding Shape (Spectral + Labels): (510, 203)\n",
      "  -> SVM (RBF) + Scaler: Train Acc=0.9922, Test Acc=0.4556, F1=0.4615\n",
      "  -> Random Forest: Train Acc=1.0000, Test Acc=0.5556, F1=0.5584\n",
      "  -> MLP: Train Acc=0.9137, Test Acc=0.4444, F1=0.4398\n",
      "\n",
      "================================================================================\n",
      "Experiment 2/2: Function='polynomial', Bins=100, Range=4.1 + Node Labels\n",
      "================================================================================\n",
      "Total Embedding Shape (Spectral + Labels): (510, 103)\n",
      "  -> SVM (RBF) + Scaler: Train Acc=0.9882, Test Acc=0.4333, F1=0.4392\n",
      "  -> Random Forest: Train Acc=1.0000, Test Acc=0.5444, F1=0.5429\n",
      "  -> MLP: Train Acc=0.9451, Test Acc=0.4778, F1=0.4794\n",
      "\n",
      "========================================================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "========================================================================================================================\n",
      "Func         Parameters                     Classifier           Train Acc   Test Acc   F1         GenTime \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "hybrid       h_bins=100,h_range=33,p_bins=100,p_range=4.1 Random Forest        1.0000      0.5556     0.5584     4.46    \n",
      "polynomial   bins=100, range=4.1            Random Forest        1.0000      0.5444     0.5429     3.51    \n",
      "polynomial   bins=100, range=4.1            MLP                  0.9451      0.4778     0.4794     3.51    \n",
      "hybrid       h_bins=100,h_range=33,p_bins=100,p_range=4.1 SVM (RBF) + Scaler   0.9922      0.4556     0.4615     4.46    \n",
      "hybrid       h_bins=100,h_range=33,p_bins=100,p_range=4.1 MLP                  0.9137      0.4444     0.4398     4.46    \n",
      "polynomial   bins=100, range=4.1            SVM (RBF) + Scaler   0.9882      0.4333     0.4392     3.51    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "configs = [\n",
    "    {'func': 'hybrid', 'harm_bins': 100, 'harm_range': 33, 'pol_bins': 100, 'pol_range': 4.1},\n",
    "    {'func': 'polynomial', 'bins': 100, 'range': 4.1},\n",
    "]\n",
    "\n",
    "print(\"Starting Multi-Configuration FGSD Experiment...\")\n",
    "results = run_experiment(configs)\n",
    "print_summary(results)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"fgsd_triplets_labels.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
