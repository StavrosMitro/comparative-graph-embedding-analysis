{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a79f4b9",
   "metadata": {},
   "source": [
    "# FGSD Experiment on REDDIT-MULTI-12K\n",
    "This notebook runs the Flexible FGSD and Hybrid FGSD experiments on the REDDIT-MULTI-12K dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadee52",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "091b17af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added to path: /home/stavros/emb3/fgsd_method/src\n",
      "✅ Torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Workaround for MKL linking issues (iJIT_NotifyEvent error)\n",
    "# This must be set before importing torch or numpy\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "\n",
    "import torch # Import torch first\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "print(f\"✅ Added to path: {parent_dir}\")\n",
    "print(f\"✅ Torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe9e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Warning: torch_geometric import failed: partially initialized module 'torch_geometric' has no attribute 'typing' (most likely due to a circular import)\n",
      "Will attempt to use memory-efficient manual loading if TUDataset fails.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Check for PyG\n",
    "try:\n",
    "    # torch is already imported in the previous cell\n",
    "    from torch_geometric.datasets import TUDataset\n",
    "    from torch_geometric.utils import to_networkx\n",
    "    HAS_PYG = True\n",
    "    print(\"✅ torch_geometric is available. TUDataset will be used.\")\n",
    "except (ImportError, AttributeError) as e:\n",
    "    print(f\"❌ Warning: torch_geometric import failed: {e}\")\n",
    "    print(\"Will attempt to use memory-efficient manual loading if TUDataset fails.\")\n",
    "    HAS_PYG = False\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import tracemalloc\n",
    "import gc\n",
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Iterator, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from fgsd import FlexibleFGSD\n",
    "from optimized_method import HybridFGSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e2d2b",
   "metadata": {},
   "source": [
    "## Constants and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b4deff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for batch processing\n",
    "BATCH_SIZE = 500\n",
    "DATASET_DIR = '/tmp/REDDIT-MULTI-12K'\n",
    "\n",
    "@dataclass\n",
    "class GraphRecord:\n",
    "    \"\"\"Metadata for a single graph.\"\"\"\n",
    "    graph_id: int\n",
    "    label: int\n",
    "    node_start: int\n",
    "    node_end: int\n",
    "\n",
    "def ensure_dataset_ready():\n",
    "    \"\"\"Ensure the dataset is downloaded and extracted.\"\"\"\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    \n",
    "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "    base_url = 'https://www.chrsmrrs.com/graphkerneldatasets/REDDIT-MULTI-12K.zip'\n",
    "    zip_path = os.path.join(DATASET_DIR, 'REDDIT-MULTI-12K.zip')\n",
    "    dataset_path = os.path.join(DATASET_DIR, 'REDDIT-MULTI-12K')\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Downloading REDDIT-MULTI-12K dataset...\")\n",
    "        urllib.request.urlretrieve(base_url, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATASET_DIR)\n",
    "        print(\"Download complete.\")\n",
    "    return dataset_path\n",
    "\n",
    "def load_metadata(dataset_dir: str) -> List[GraphRecord]:\n",
    "    \"\"\"Load graph metadata without loading full graphs into memory.\"\"\"\n",
    "    dataset_path = os.path.join(dataset_dir, 'REDDIT-MULTI-12K')\n",
    "    \n",
    "    graph_indicator = pd.read_csv(\n",
    "        os.path.join(dataset_path, 'REDDIT-MULTI-12K_graph_indicator.txt'), \n",
    "        header=None\n",
    "    )[0].values\n",
    "    graph_labels = pd.read_csv(\n",
    "        os.path.join(dataset_path, 'REDDIT-MULTI-12K_graph_labels.txt'), \n",
    "        header=None\n",
    "    )[0].values\n",
    "    \n",
    "    records = []\n",
    "    num_graphs = len(graph_labels)\n",
    "    \n",
    "    for gid in range(1, num_graphs + 1):\n",
    "        mask = graph_indicator == gid\n",
    "        indices = np.where(mask)[0]\n",
    "        if len(indices) > 0:\n",
    "            node_start = indices[0]\n",
    "            node_end = indices[-1] + 1\n",
    "        else:\n",
    "            node_start = node_end = 0\n",
    "        \n",
    "        records.append(GraphRecord(\n",
    "            graph_id=gid,\n",
    "            label=graph_labels[gid - 1] - 1,  # 0-indexed\n",
    "            node_start=node_start,\n",
    "            node_end=node_end\n",
    "        ))\n",
    "    \n",
    "    return records\n",
    "\n",
    "def iter_graph_batches(dataset_dir: str, batch_size: int, records: Optional[List[GraphRecord]] = None) -> Iterator[Tuple[List[nx.Graph], np.ndarray, List[int]]]:\n",
    "    \"\"\"\n",
    "    Iterate over graphs in batches, loading only what's needed.\n",
    "    Yields: (list of graphs, labels array, graph_ids list)\n",
    "    \"\"\"\n",
    "    dataset_path = os.path.join(dataset_dir, 'REDDIT-MULTI-12K')\n",
    "    \n",
    "    if records is None:\n",
    "        records = load_metadata(dataset_dir)\n",
    "    \n",
    "    # Load edges once (unavoidable for graph construction)\n",
    "    edges_df = pd.read_csv(\n",
    "        os.path.join(dataset_path, 'REDDIT-MULTI-12K_A.txt'),\n",
    "        header=None, names=['src', 'dst']\n",
    "    )\n",
    "    graph_indicator = pd.read_csv(\n",
    "        os.path.join(dataset_path, 'REDDIT-MULTI-12K_graph_indicator.txt'),\n",
    "        header=None\n",
    "    )[0].values\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(records), batch_size):\n",
    "        batch_records = records[i:i + batch_size]\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        gids = []\n",
    "        \n",
    "        for rec in batch_records:\n",
    "            G = nx.Graph()\n",
    "            # Add nodes for this graph\n",
    "            node_ids = np.where(graph_indicator == rec.graph_id)[0] + 1  # 1-indexed\n",
    "            for nid in node_ids:\n",
    "                G.add_node(nid)\n",
    "            \n",
    "            # Add edges for this graph\n",
    "            mask = graph_indicator[edges_df['src'].values - 1] == rec.graph_id\n",
    "            graph_edges = edges_df[mask][['src', 'dst']].values\n",
    "            G.add_edges_from(graph_edges)\n",
    "            \n",
    "            # Relabel to 0-indexed\n",
    "            G = nx.convert_node_labels_to_integers(G)\n",
    "            \n",
    "            graphs.append(G)\n",
    "            labels.append(rec.label)\n",
    "            gids.append(rec.graph_id)\n",
    "        \n",
    "        yield graphs, np.array(labels), gids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f479b9",
   "metadata": {},
   "source": [
    "## Download and Load REDDIT-MULTI-12K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38ea1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load_reddit():\n",
    "    \"\"\"\n",
    "    Download and load REDDIT-MULTI-12K dataset.\n",
    "    Prioritizes TUDataset for efficiency, falls back to optimized manual loading.\n",
    "    \"\"\"\n",
    "    # Option 1: Try TUDataset (Preferred)\n",
    "    if HAS_PYG:\n",
    "        try:\n",
    "            root_dir = '/tmp/TUDataset'\n",
    "            os.makedirs(root_dir, exist_ok=True)\n",
    "            print(f\"Loading REDDIT-MULTI-12K via TUDataset (root={root_dir})...\")\n",
    "            dataset = TUDataset(root=root_dir, name='REDDIT-MULTI-12K')\n",
    "            \n",
    "            print(f\"Dataset loaded. Converting {len(dataset)} graphs to NetworkX...\")\n",
    "            graphs = []\n",
    "            labels = []\n",
    "            for data in dataset:\n",
    "                g = to_networkx(data, to_undirected=True)\n",
    "                graphs.append(g)\n",
    "                labels.append(data.y.item())\n",
    "            \n",
    "            labels = np.array(labels)\n",
    "            if labels.min() > 0: labels = labels - labels.min()\n",
    "            \n",
    "            print(f\"Successfully loaded {len(graphs)} graphs via TUDataset.\")\n",
    "            return graphs, labels\n",
    "        except Exception as e:\n",
    "            print(f\"❌ TUDataset loading failed: {e}\")\n",
    "            print(\"Falling back to manual loading.\")\n",
    "    else:\n",
    "        print(\"❌ torch_geometric not available. Skipping TUDataset.\")\n",
    "\n",
    "    # Option 2: Memory-Efficient Manual Loading (Fallback)\n",
    "    print(\"Using memory-efficient manual loading...\")\n",
    "    data_dir = '/tmp/REDDIT-MULTI-12K'\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    base_url = 'https://www.chrsmrrs.com/graphkerneldatasets/REDDIT-MULTI-12K.zip'\n",
    "    zip_path = os.path.join(data_dir, 'REDDIT-MULTI-12K.zip')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir, 'REDDIT-MULTI-12K')):\n",
    "        print(\"Downloading REDDIT-MULTI-12K dataset...\")\n",
    "        import urllib.request\n",
    "        import zipfile\n",
    "        urllib.request.urlretrieve(base_url, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "            \n",
    "    dataset_path = os.path.join(data_dir, 'REDDIT-MULTI-12K')\n",
    "    \n",
    "    # Use pandas for memory efficient reading (C engine)\n",
    "    print(\"Reading graph indicators...\")\n",
    "    graph_indicator = pd.read_csv(os.path.join(dataset_path, 'REDDIT-MULTI-12K_graph_indicator.txt'), header=None)[0].values\n",
    "    \n",
    "    print(\"Reading graph labels...\")\n",
    "    graph_labels = pd.read_csv(os.path.join(dataset_path, 'REDDIT-MULTI-12K_graph_labels.txt'), header=None)[0].values\n",
    "    \n",
    "    num_graphs = len(graph_labels)\n",
    "    graphs = [nx.Graph() for _ in range(num_graphs)]\n",
    "    \n",
    "    # Add nodes\n",
    "    for node_id, graph_id in enumerate(graph_indicator, start=1):\n",
    "        graphs[graph_id - 1].add_node(node_id)\n",
    "        \n",
    "    print(\"Reading edges (chunked)...\")\n",
    "    # Read edges in chunks to save RAM\n",
    "    edge_file = os.path.join(dataset_path, 'REDDIT-MULTI-12K_A.txt')\n",
    "    chunksize = 100000\n",
    "    \n",
    "    for chunk in pd.read_csv(edge_file, header=None, delimiter=',', chunksize=chunksize):\n",
    "        edges = chunk.values\n",
    "        # Vectorized lookup: edges[:,0] is u (1-based), so u-1 is index in graph_indicator\n",
    "        u_indices = edges[:, 0] - 1\n",
    "        target_graphs = graph_indicator[u_indices]\n",
    "        \n",
    "        # Iterate and add\n",
    "        for i in range(len(edges)):\n",
    "            u, v = edges[i]\n",
    "            graph_id = target_graphs[i]\n",
    "            graphs[graph_id - 1].add_edge(u, v)\n",
    "            \n",
    "    # Relabel\n",
    "    graphs = [nx.convert_node_labels_to_integers(g) for g in graphs]\n",
    "    labels = graph_labels - 1\n",
    "    \n",
    "    return graphs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962949a",
   "metadata": {},
   "source": [
    "PREANALYSIS FOR BINS AND RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "779e45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_spectral_distances_for_func(graphs, func_type, sample_size=500):\n",
    "    \"\"\"\n",
    "    Computes spectral distances based on function f(lambda).\n",
    "    Uses a random sample of graphs to avoid processing everything for pre-analysis.\n",
    "    \"\"\"\n",
    "    print(f\"Computing spectral distances using func='{func_type}' (Sample size={sample_size})...\")\n",
    "    \n",
    "    # Sample graphs if needed\n",
    "    np.random.seed(42)\n",
    "    if len(graphs) > sample_size:\n",
    "        sample_indices = np.random.choice(len(graphs), sample_size, replace=False)\n",
    "        sample_graphs = [graphs[i] for i in sample_indices]\n",
    "    else:\n",
    "        sample_graphs = graphs\n",
    "    \n",
    "    all_distances = []\n",
    "    node_counts = []\n",
    "\n",
    "    for G in sample_graphs:\n",
    "        if G.number_of_nodes() < 2:\n",
    "            continue\n",
    "        node_counts.append(G.number_of_nodes())\n",
    "        try:\n",
    "            # 1. Normalized Laplacian\n",
    "            L = np.asarray(nx.normalized_laplacian_matrix(G).todense())\n",
    "            # 2. Eigen-decomposition\n",
    "            w, v = np.linalg.eigh(L)\n",
    "            # 3. Apply function f(lambda)\n",
    "            if func_type == 'harmonic':\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    func_w = np.where(w > 1e-9, 1.0 / w, 0)\n",
    "            elif func_type == 'polynomial':\n",
    "                func_w = w ** 2\n",
    "            elif func_type == 'biharmonic':\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    func_w = np.where(w > 1e-9, 1.0 / (w**2), 0)\n",
    "            else:\n",
    "                func_w = w\n",
    "            \n",
    "            # 4. Reconstruct f(L)\n",
    "            fL = v @ np.diag(func_w) @ v.T\n",
    "            # 5. Compute Distances\n",
    "            ones = np.ones(L.shape[0])\n",
    "            S = np.outer(np.diag(fL), ones) + np.outer(ones, np.diag(fL)) - 2 * fL\n",
    "            S = np.asarray(S)\n",
    "            # Extract upper triangle distances\n",
    "            triu_indices = np.triu_indices_from(S, k=1)\n",
    "            distances = S[triu_indices]\n",
    "            all_distances.extend(distances.flatten().tolist())\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return np.array(all_distances), np.array(node_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe37c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def analyze_and_visualize(distances, node_counts, func_type):\n",
    "    \"\"\"\n",
    "    Prints statistics and saves the plot.\n",
    "    \"\"\"\n",
    "    save_path = f'reddit_analysis_{func_type}.png'\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"PRE-ANALYSIS RESULTS FOR: {func_type.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    # --- RANGE ANALYSIS ---\n",
    "    min_val = np.min(distances)\n",
    "    max_val = np.max(distances)\n",
    "    p95 = np.percentile(distances, 95)\n",
    "    p99 = np.percentile(distances, 99)\n",
    "    print(f\"1. SPECTRAL DISTANCE VALUES (hist_range)\")\n",
    "    print(f\"   Min:  {min_val:.4f}\")\n",
    "    print(f\"   Max:  {max_val:.4f}\")\n",
    "    print(f\"   95th Percentile: {p95:.4f}\")\n",
    "    print(f\"   99th Percentile: {p99:.4f}  <-- RECOMMENDED Range\")\n",
    "    # --- SPARSITY ANALYSIS ---\n",
    "    print(\"\\n2. SPARSITY CHECK (hist_bins)\")\n",
    "    sim_range = p99\n",
    "    test_bins = [50, 100, 200, 300, 500]\n",
    "    print(f\"   Assume Range = [0, {sim_range:.2f}]\")\n",
    "    print(f\"   {'Bins':<10} | {'Avg Value/Bin':<15} | {'Sparsity Risk'}\")\n",
    "    print(\"   \" + \"-\"*45)\n",
    "    for b in test_bins:\n",
    "        hist, _ = np.histogram(distances, bins=b, range=(0, sim_range))\n",
    "        avg_hits_per_graph = np.sum(hist) / len(node_counts) / b\n",
    "        risk = \"Low\"\n",
    "        if avg_hits_per_graph < 1.0: risk = \"Medium\"\n",
    "        if avg_hits_per_graph < 0.1: risk = \"HIGH (Too Sparse)\"\n",
    "        print(f\"   {b:<10} | {avg_hits_per_graph:.4f}{'':<10} | {risk}\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    viz_cutoff = np.percentile(distances, 99.5)\n",
    "    viz_data = distances[distances <= viz_cutoff]\n",
    "    \n",
    "    ax.hist(viz_data, bins=100, color='orange', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(p95, color='blue', linestyle='--', label=f'95% ({p95:.1f})')\n",
    "    ax.axvline(p99, color='darkred', linestyle='-', label=f'99% ({p99:.1f})')\n",
    "    \n",
    "    ax.set_title(f\"REDDIT-MULTI-12K Distribution: {func_type} distances\")\n",
    "    ax.set_xlabel(\"Spectral Distance Value\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"\\nPlot saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751dc486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ torch_geometric not available. Skipping TUDataset.\n",
      "Using memory-efficient manual loading...\n",
      "Reading graph indicators...\n",
      "Reading graph labels...\n",
      "Reading edges (chunked)...\n",
      "Loaded 11929 graphs.\n",
      "Computing spectral distances using func='harmonic' (Sample size=500)...\n",
      "\n",
      "============================================================\n",
      "PRE-ANALYSIS RESULTS FOR: HARMONIC\n",
      "============================================================\n",
      "1. SPECTRAL DISTANCE VALUES (hist_range)\n",
      "   Min:  0.2500\n",
      "   Max:  87.6544\n",
      "   95th Percentile: 7.2961\n",
      "   99th Percentile: 11.6000  <-- RECOMMENDED Range\n",
      "\n",
      "2. SPARSITY CHECK (hist_bins)\n",
      "   Assume Range = [0, 11.60]\n",
      "   Bins       | Avg Value/Bin   | Sparsity Risk\n",
      "   ---------------------------------------------\n",
      "   50         | 2912.5661           | Low\n",
      "   100        | 1456.2830           | Low\n",
      "   200        | 728.1415           | Low\n",
      "   300        | 485.4277           | Low\n",
      "   500        | 291.2566           | Low\n",
      "\n",
      "Plot saved to: reddit_analysis_harmonic.png\n",
      "Computing spectral distances using func='polynomial' (Sample size=500)...\n",
      "\n",
      "============================================================\n",
      "PRE-ANALYSIS RESULTS FOR: POLYNOMIAL\n",
      "============================================================\n",
      "1. SPECTRAL DISTANCE VALUES (hist_range)\n",
      "   Min:  1.0017\n",
      "   Max:  8.0000\n",
      "   95th Percentile: 3.0105\n",
      "   99th Percentile: 3.3189  <-- RECOMMENDED Range\n",
      "\n",
      "2. SPARSITY CHECK (hist_bins)\n",
      "   Assume Range = [0, 3.32]\n",
      "   Bins       | Avg Value/Bin   | Sparsity Risk\n",
      "   ---------------------------------------------\n",
      "   50         | 2912.5660           | Low\n",
      "   100        | 1456.2830           | Low\n",
      "   200        | 728.1415           | Low\n",
      "   300        | 485.4277           | Low\n",
      "   500        | 291.2566           | Low\n",
      "\n",
      "Plot saved to: reddit_analysis_polynomial.png\n",
      "Computing spectral distances using func='biharmonic' (Sample size=500)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load Data\n",
    "graphs, labels = download_and_load_reddit()\n",
    "print(f\"Loaded {len(graphs)} graphs.\")\n",
    "\n",
    "# 2. Define functions to test\n",
    "functions_to_test = ['harmonic', 'polynomial', 'biharmonic']\n",
    "\n",
    "# 3. Run Loop\n",
    "for func in functions_to_test:\n",
    "    distances, node_counts = compute_spectral_distances_for_func(graphs, func_type=func)\n",
    "    analyze_and_visualize(distances, node_counts, func_type=func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88864fd1",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(X_train, X_test, y_train, y_test, classifier_name, clf):\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    try:\n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_score = clf.predict_proba(X_test)\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            y_score = clf.decision_function(X_test)\n",
    "            if len(y_score.shape) == 1:\n",
    "                y_score = y_score.reshape(-1, 1)\n",
    "        else:\n",
    "            y_score = None\n",
    "\n",
    "        if y_score is not None and y_test_bin.shape[1] > 1:\n",
    "            auc = roc_auc_score(y_test_bin, y_score, average='weighted', multi_class='ovr')\n",
    "        else:\n",
    "            auc = None\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    return {\n",
    "        'classifier': classifier_name,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'train_time': train_time,\n",
    "        'inference_time': inference_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a7f55",
   "metadata": {},
   "source": [
    "## Run FGSD Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de386108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(configs, test_size=0.15, random_state=42):\n",
    "    print(\"Ensuring dataset is ready on disk...\")\n",
    "    ensure_dataset_ready()\n",
    "    \n",
    "    records = load_metadata(DATASET_DIR)\n",
    "    all_labels = np.array([r.label for r in records])\n",
    "    all_gids = np.array([r.graph_id for r in records])\n",
    "    \n",
    "    # Pre-calculate Stratified Split Indices\n",
    "    # We split indices, not graphs.\n",
    "    indices = np.arange(len(records))\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        indices, test_size=test_size, random_state=random_state, stratify=all_labels\n",
    "    )\n",
    "    \n",
    "    # Create mask for fast checking\n",
    "    is_train = np.zeros(len(records), dtype=bool)\n",
    "    is_train[train_idx] = True\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for i, config in enumerate(configs):\n",
    "        func = config['func']\n",
    "        \n",
    "        print(f\"\\nExperiment {i+1}/{len(configs)}: {func}\")\n",
    "        print(f\"Generating embeddings batch-wise to save RAM...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # We need to compute embeddings for ALL graphs.\n",
    "        # We can't reuse one model instance easily if the model stores state during fit.\n",
    "        # However, FGSD/HybridFGSD usually don't learn parameters from other graphs (it's a histogram).\n",
    "        # We will instantiate the model and process batch by batch.\n",
    "        \n",
    "        if func == 'hybrid':\n",
    "            model = HybridFGSD(\n",
    "                harm_bins=config.get('harm_bins', 200), harm_range=config.get('harm_range', 20),\n",
    "                pol_bins=config.get('pol_bins', 70), pol_range=config.get('pol_range', 4.1),\n",
    "                func_type='hybrid', seed=random_state\n",
    "            )\n",
    "        else:\n",
    "            model = FlexibleFGSD(hist_bins=config['bins'], hist_range=config['range'], func_type=func, seed=random_state)\n",
    "\n",
    "        # Storage for embeddings\n",
    "        # We don't know embedding size until we run first graph, but usually it's bins or bins*2\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for graphs, labels, _ in tqdm(iter_graph_batches(DATASET_DIR, BATCH_SIZE), desc=\"Embedding\"):\n",
    "            # Compute embeddings for this batch\n",
    "            # We fit the model on the batch. Since FGSD is instance-based, this is valid.\n",
    "            model.fit(graphs)\n",
    "            batch_emb = model.get_embedding() # numpy array [batch, dim]\n",
    "            embeddings_list.append(batch_emb)\n",
    "            \n",
    "            del graphs\n",
    "            gc.collect()\n",
    "            \n",
    "        # Concatenate all embeddings\n",
    "        X_all = np.vstack(embeddings_list)\n",
    "        y_all = all_labels # aligns with records\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        print(f\"Embedding Complete. Shape: {X_all.shape}\")\n",
    "\n",
    "        # Split based on pre-calculated indices\n",
    "        X_train = X_all[train_idx]\n",
    "        y_train = y_all[train_idx]\n",
    "        X_test = X_all[test_idx]\n",
    "        y_test = y_all[test_idx]\n",
    "\n",
    "        classifiers = {\n",
    "            'SVM (RBF) + Scaler': make_pipeline(\n",
    "                StandardScaler(),\n",
    "                SVC(kernel='rbf', C=100, gamma='scale', probability=True, random_state=random_state)\n",
    "            ),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=1000, random_state=random_state, n_jobs=-1),\n",
    "            'MLP': make_pipeline(\n",
    "                StandardScaler(), \n",
    "                MLPClassifier(hidden_layer_sizes=(1024, 512, 256, 128), max_iter=1000, early_stopping=True, random_state=random_state)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            res = evaluate_classifier(X_train, X_test, y_train, y_test, clf_name, clf)\n",
    "            res.update(config)\n",
    "            res['generation_time'] = generation_time\n",
    "            results.append(res)\n",
    "            print(f\"  -> {clf_name}: Test Acc={res['accuracy']:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438d470",
   "metadata": {},
   "source": [
    "## Summary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(results):\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"SUMMARY OF RESULTS\")\n",
    "    print(\"=\"*120)\n",
    "    print(f\"{'Func':<12} {'Parameters':<30} {'Classifier':<20} {'Train Acc':<11} {'Test Acc':<10} {'F1':<10} {'GenTime':<8}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "    for r in sorted_results:\n",
    "        if r['func'] == 'hybrid':\n",
    "            params = f\"h_bins={r.get('harm_bins')},h_range={r.get('harm_range')},p_bins={r.get('pol_bins')},p_range={r.get('pol_range')}\"\n",
    "        else:\n",
    "            params = f\"bins={r.get('bins')}, range={r.get('range')}\"\n",
    "\n",
    "        print(f\"{r['func']:<12} {params:<30} {r['classifier']:<20} \"\n",
    "              f\"{r['train_accuracy']:<11.4f} {r['accuracy']:<10.4f} {r['f1_score']:<10.4f} {r['generation_time']:<8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d491fd",
   "metadata": {},
   "source": [
    "## Run the Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a6165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multi-Configuration FGSD Experiment on REDDIT-MULTI-12K...\n",
      "Loading REDDIT-MULTI-12K dataset...\n",
      "Downloading REDDIT-MULTI-12K dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multi-Configuration FGSD Experiment on REDDIT-MULTI-12K...\n",
      "Loading REDDIT-MULTI-12K dataset...\n",
      "Downloading REDDIT-MULTI-12K dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m configs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mharm_bins\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mharm_range\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpol_bins\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpol_range\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3.5\u001b[39m},\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolynomial\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbins\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3.1\u001b[39m},\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mharmonic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbins\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3.5\u001b[39m},\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Multi-Configuration FGSD Experiment on REDDIT-MULTI-12K...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m print_summary(results)\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(configs, test_size, random_state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_experiment\u001b[39m(configs, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading REDDIT-MULTI-12K dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     graphs, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_load_reddit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREDDIT-MULTI-12K has no node labels. Using only spectral features.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     graphs_train, graphs_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      8\u001b[0m         graphs, labels, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state, stratify\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m      9\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mdownload_and_load_reddit\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREDDIT-MULTI-12K\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading REDDIT-MULTI-12K dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(zip_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     12\u001b[0m         zip_ref\u001b[38;5;241m.\u001b[39mextractall(data_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/urllib/request.py:268\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    265\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/ssl.py:1277\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1274\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1275\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/ssl.py:1135\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "configs = [\n",
    "    {'func': 'hybrid', 'harm_bins': 100, 'harm_range': 3.5, 'pol_bins': 200, 'pol_range': 3.5},\n",
    "    {'func': 'polynomial', 'bins': 200, 'range': 3.1},\n",
    "    {'func': 'harmonic', 'bins': 100, 'range': 3.5},\n",
    "]\n",
    "\n",
    "print(\"Starting Multi-Configuration FGSD Experiment on REDDIT-MULTI-12K...\")\n",
    "results = run_experiment(configs)\n",
    "print_summary(results)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"fgsd_reddit_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
